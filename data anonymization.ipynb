{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anonymizing data with Faker\n",
    "\n",
    "https://faker.readthedocs.io/en/master/\n",
    "\n",
    "This is from a real example, so there is some wrangling and merging at the top which isn't relevant. I'll do a clean gist on this someday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from faker import Faker\n",
    "\n",
    "# date and time handling\n",
    "from datetime import datetime, date\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv.  encoding gets around an error thrown by reading in with default utf-8\n",
    "df = pd.read_csv('ssw data1-22-19.csv', encoding='ISO-8859-1')\n",
    "users = pd.read_csv('users data1-22-19.csv')\n",
    "roles = pd.read_csv('userRole update.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, users, left_on='user_id', right_on='ID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, roles, left_on='user_login', right_on='NetID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove template, plugins_group, new_users, new_users_role because never implemented in SSW\n",
    "# keeping user_login from the users df\n",
    "# keeping current primary role from roles df\n",
    "df = df[['ssw_id', 'user_id', 'user_login', 'u_role', 'u_division', 'title',\n",
    "       'privacy', 'template', 'theme', 'plugins_list',\n",
    "        'next_stage', 'site_type', 'blog_id',\n",
    "       'ssw_main_meta', 'site_created', 'wizard_completed', 'starttime',\n",
    "       'endtime']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_login = df.user_login.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_login = pd.DataFrame(uni_login, dtype='string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_login.columns = ['user_login']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used this code (probably from StackExchange) for reference.\n",
    "\n",
    "```\n",
    "import unicodecsv as csv\n",
    "from faker import Faker\n",
    "from collections import defaultdict\n",
    "def anonymize_rows(rows):\n",
    "    \"\"\"\n",
    "    Rows is an iterable of dictionaries that contain name and\n",
    "    email fields that need to be anonymized.\n",
    "    \"\"\"\n",
    "    # Load the faker and its providers\n",
    "    faker  = Factory.create()\n",
    "    # Create mappings of names & emails to faked names & emails.\n",
    "    names  = defaultdict(faker.name)\n",
    "    emails = defaultdict(faker.email)\n",
    "    # Iterate over the rows and yield anonymized rows.\n",
    "    for row in rows:\n",
    "        # Replace the name and email fields with faked fields.\n",
    "        row['name']  = names[row['name']]\n",
    "        row['email'] = emails[row['email']]\n",
    "        # Yield the row back to the caller\n",
    "        yield row\n",
    "\n",
    "def anonymize(source, target):\n",
    "    \"\"\"\n",
    "    The source argument is a path to a CSV file containing data to anonymize,\n",
    "    while target is a path to write the anonymized CSV data to.\n",
    "    \"\"\"\n",
    "    with open(source, 'rU') as f:\n",
    "        with open(target, 'w') as o:\n",
    "            # Use the DictReader to easily extract fields\n",
    "            reader = csv.DictReader(f)\n",
    "            writer = csv.DictWriter(o, reader.fieldnames)\n",
    "            # Read and anonymize data, writing to target file.\n",
    "            for row in anonymize_rows(reader):\n",
    "                writer.writerow(row)\n",
    "```\n",
    "\n",
    "The way I actually implemented was to create a function which created anonymized data for each row.  In this first case, I just fed it a dataframe with a single column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anonymize_login(x):\n",
    "    # Load the faker\n",
    "    fake = Faker()\n",
    "    # Create an empty array where we can store the fake names.\n",
    "    fake_login = []\n",
    "    # for each row, create a fake user name\n",
    "    for rows in x['user_login']:\n",
    "        fake_login.append(fake.user_name())\n",
    "    \n",
    "    # create a new column and assign the array\n",
    "    x['anon_login'] = fake_login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I run the function with my single-column df. It returns with the anonymized values. I merge on the true login data and then drop it, leaving the anonymized field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "anonymize_login(uni_login)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, uni_login, on='user_login', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['user_login'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to change site titles. \n",
    "# Same concept as above, I just don't bother with a separate df.\n",
    "# I overwrite the original data as I go.\n",
    "# In the future, this is the way to do it.\n",
    "def anonymize_title(x):\n",
    "    fake = Faker()\n",
    "    fake_title = []\n",
    "    \n",
    "    for rows in x['title']:\n",
    "        fake_title.append(fake.text(max_nb_chars=70, ext_word_list=None))\n",
    "        \n",
    "    x['title'] = fake_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "anonymize_title(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('anon_data', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
